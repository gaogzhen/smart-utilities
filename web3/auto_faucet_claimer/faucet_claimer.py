# faucet_claimer.py
import os
import sys

# åœ¨å¯¼å…¥ crawl4ai/playwright ä¹‹å‰æŒ‡å®šæµè§ˆå™¨è·¯å¾„ï¼Œä½¿ç”¨é¡¹ç›®å†…ç›®å½•ï¼Œé¿å…ä¸ç³»ç»Ÿç¼“å­˜è·¯å¾„ä¸ä¸€è‡´å¯¼è‡´æ‰¾ä¸åˆ°æµè§ˆå™¨
_script_dir = os.path.dirname(os.path.abspath(__file__))
os.environ.setdefault("PLAYWRIGHT_BROWSERS_PATH", os.path.join(_script_dir, ".playwright-browsers"))

import asyncio
import json
import time
from datetime import datetime, timedelta
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from config import FAUCET_TASKS, WALLET_ADDRESS, PROXY


class FaucetClaimer:
    def __init__(self):
        # åŠ è½½é¢†å–å†å²è®°å½•
        self.history_file = "claim_history.json"
        self.history = self.load_history()
        self.browser_config = BrowserConfig(
            headless=False,  # é¦–æ¬¡è°ƒè¯•è®¾ä¸ºFalseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œï¼Œç¨³å®šåæ”¹ä¸ºTrue
            proxy=PROXY if PROXY else None
        )

    def load_history(self):
        """åŠ è½½å†å²é¢†å–è®°å½•"""
        try:
            with open(self.history_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}  # é¦–æ¬¡è¿è¡Œï¼Œå†å²è®°å½•ä¸ºç©º

    def save_history(self):
        """ä¿å­˜å†å²é¢†å–è®°å½•"""
        with open(self.history_file, 'w') as f:
            json.dump(self.history, f, indent=4)

    def can_claim(self, faucet_name):
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³24å°æ—¶é—´éš”"""
        if faucet_name not in self.history:
            return True
        last_claimed_str = self.history[faucet_name].get("last_claimed")
        if not last_claimed_str:
            return True

        last_claimed = datetime.fromisoformat(last_claimed_str)
        time_since_last = datetime.now() - last_claimed
        return time_since_last > timedelta(hours=24)

    async def execute_steps(self, page, steps, faucet_name):
        """æ‰§è¡Œé¢„å®šä¹‰çš„æ­¥éª¤åºåˆ—ã€‚page ä¸º Playwright Page å¯¹è±¡ï¼ˆç”± _get_page(crawler) è·å–ï¼‰ã€‚"""
        if not page:
            print("  âŒ æ— æ³•è·å–å½“å‰é¡µé¢ï¼Œè·³è¿‡æ­¥éª¤")
            return
        for step in steps:
            print(f"  -> æ­¥éª¤: {step.get('description', 'N/A')}")
            action = step.get("action")

            if action == "type":
                selector = step["selector"]
                value = step["value"]
                await page.type(selector, value)
                await asyncio.sleep(1)

            elif action == "click":
                selector = step["selector"]
                await page.click(selector)
                await asyncio.sleep(2)

            elif action == "select":
                selector = step["selector"]
                value = step["value"]
                await page.select_option(selector, value)
                await asyncio.sleep(1)

            elif action == "wait_for_text":
                text = step["text"]
                timeout_ms = step.get("timeout", 10) * 1000
                try:
                    await page.get_by_text(text).first.wait_for(state="visible", timeout=timeout_ms)
                except Exception:
                    print(f"    è­¦å‘Š: æœªåœ¨ {timeout_ms // 1000}s å†…æ‰¾åˆ°æ–‡æœ¬ '{text}'")

            elif action == "solve_captcha":
                # é‡åˆ°éªŒè¯ç æ—¶ï¼Œæš‚åœè„šæœ¬ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ
                print(f"\nâš ï¸  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨è§£å†³éªŒè¯ç  ({faucet_name})...")
                print("    è§£å†³åï¼Œè¯·åœ¨æ§åˆ¶å°æŒ‰å›è½¦é”®ç»§ç»­...")
                input()  # é˜»å¡ï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ“ä½œåæŒ‰å›è½¦
                await asyncio.sleep(3)

            await asyncio.sleep(1)  # æ­¥éª¤é—´åŸºç¡€é—´éš”

    def check_success(self, result_or_html, success_indicators):
        """æ ¹æ®é…ç½®çš„æˆåŠŸæŒ‡æ ‡ï¼Œåˆ¤æ–­é¢†å–æ˜¯å¦æˆåŠŸã€‚result_or_html å¯ä¸ºå¸¦ .html çš„å¯¹è±¡æˆ– HTML å­—ç¬¦ä¸²ã€‚"""
        html = result_or_html.html if hasattr(result_or_html, "html") else result_or_html
        html_lower = (html or "").lower()
        for indicator in success_indicators:
            ind_type = indicator["type"]

            if ind_type == "text_in_page":
                if indicator["content"].lower() in html_lower:
                    return True

            elif ind_type == "element_present":
                if indicator["selector"] in (html or ""):
                    return True
        return False

    async def claim_single_faucet(self, faucet):
        """é¢†å–å•ä¸ªæ°´é¾™å¤´"""
        faucet_name = faucet["name"]

        # 1. æ£€æŸ¥æ—¶é—´é—´éš”
        if not self.can_claim(faucet_name):
            wait_until = datetime.fromisoformat(self.history[faucet_name]["last_claimed"]) + timedelta(hours=24)
            print(f"â¸ï¸  è·³è¿‡ {faucet_name}ï¼Œä¸‹æ¬¡å¯é¢†å–æ—¶é—´: {wait_until.strftime('%Y-%m-%d %H:%M')}")
            return False

        print(f"\nğŸš€ å¼€å§‹å¤„ç†: {faucet_name}")

        # 2. ä½¿ç”¨ Crawl4AI å¯åŠ¨æµè§ˆå™¨ï¼Œç›´æ¥æ‹¿ page å¹¶è‡ªè¡Œ gotoï¼Œé¿å… arun ç»“æŸåé¡µé¢è¢«å…³é—­
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            try:
                run_config = CrawlerRunConfig(url=faucet["url"])
                page, _ = await crawler.crawler_strategy.browser_manager.get_page(run_config)
                await page.goto(faucet["url"], wait_until="domcontentloaded", timeout=30000)
                await asyncio.sleep(3)  # ç­‰å¾…é¡µé¢ç¨³å®šï¼ˆå¦‚ wait_for æ•ˆæœï¼‰

                # é¢†å–æ­¥éª¤å¯èƒ½è¾ƒæ…¢ï¼ˆç½‘ç»œ/å¼¹çª—/éªŒè¯ç ï¼‰ï¼Œå°†é¡µé¢æ“ä½œé»˜è®¤è¶…æ—¶è®¾ä¸º 2 åˆ†é’Ÿ
                page.set_default_timeout(120000)

                # 3. åœ¨å½“å‰é¡µæ‰§è¡Œé¢„å®šä¹‰çš„é¢†å–æ­¥éª¤ï¼ˆpage ä¸ä¼šè¢« arun å…³é—­ï¼‰
                await self.execute_steps(page, faucet.get("steps", []), faucet_name)

                # 4. ä»å½“å‰é¡µå– HTML åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼Œä¸å†è°ƒç”¨ arun é¿å…æ–°å»º/å…³é—­é¡µé¢
                html = await page.content()
                if self.check_success(html, faucet.get("success_indicators", [])):
                    self.history[faucet_name] = {
                        "last_claimed": datetime.now().isoformat(),
                        "network": faucet["network"]
                    }
                    self.save_history()
                    print(f"âœ… æˆåŠŸé¢†å– {faucet_name}!")
                    return True
                else:
                    print(f"âš ï¸  é¢†å–å¯èƒ½æœªæˆåŠŸ: {faucet_name} (æœªæ£€æµ‹åˆ°æˆåŠŸæ ‡å¿—)")
                    return False

            except Exception as e:
                print(f"âŒ å¤„ç† {faucet_name} æ—¶å‡ºé”™: {e}")
                return False

    async def run(self):
        """éå†å¹¶å¤„ç†æ‰€æœ‰æ°´é¾™å¤´"""
        print(f"=== å¼€å§‹é¢†å–ä»»åŠ¡ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
        for faucet in FAUCET_TASKS:
            await self.claim_single_faucet(faucet)
            await asyncio.sleep(5)  # æ°´é¾™å¤´é—´é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        print("=== é¢†å–ä»»åŠ¡ç»“æŸ ===\n")


async def run_manual_demo(faucet=None):
    """
    æ‰‹åŠ¨æ¼”ç¤ºæ¨¡å¼ï¼šæ‰“å¼€æµè§ˆå™¨å¹¶åŠ è½½æ°´é¾™å¤´é¡µé¢ï¼Œæš‚åœç­‰å¾…ä½ æ‰‹åŠ¨æ“ä½œä¸€éã€‚
    æ“ä½œå®Œæˆååœ¨æ§åˆ¶å°æŒ‰å›è½¦ç»“æŸã€‚è¯·è®°ä¸‹ä½ ç‚¹å‡»çš„æŒ‰é’®æ–‡å­—ã€è¾“å…¥æ¡†ä½ç½®ç­‰ï¼Œä¾¿äºåç»­ç¼–å†™è‡ªåŠ¨åŒ–æ­¥éª¤ã€‚
    """
    faucet = faucet or (FAUCET_TASKS[0] if FAUCET_TASKS else None)
    if not faucet:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ°´é¾™å¤´ä»»åŠ¡")
        return

    print(f"\nğŸ“Œ æ‰‹åŠ¨æ¼”ç¤ºæ¨¡å¼: {faucet['name']}")
    print(f"   URL: {faucet['url']}")
    print("\n" + "=" * 60)
    print("  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆä¸€æ¬¡é¢†å–ï¼Œä¾‹å¦‚ï¼š")
    print("  1. è¿æ¥é’±åŒ…ï¼ˆå¦‚ MetaMaskï¼‰")
    print("  2. é€‰æ‹©ç½‘ç»œ / é€‰æ‹©ä»£å¸")
    print("  3. å¦‚éœ€è¦å¯è¾“å…¥æˆ–ç¡®è®¤é’±åŒ…åœ°å€")
    print("  4. å®ŒæˆäººæœºéªŒè¯ï¼ˆå¦‚æœ‰ï¼‰")
    print("  5. ç‚¹å‡» Claim é¢†å–")
    print("=" * 60)
    print("  å®Œæˆåè¯·å›åˆ°æœ¬çª—å£ï¼ŒæŒ‰ å›è½¦ ç»“æŸæ¼”ç¤ºã€‚\n")

    async with AsyncWebCrawler(config=BrowserConfig(headless=False, proxy=PROXY if PROXY else None)) as crawler:
        run_config = CrawlerRunConfig(url=faucet["url"])
        page, _ = await crawler.crawler_strategy.browser_manager.get_page(run_config)
        # æ‰‹åŠ¨æ¼”ç¤ºä¸è®¾è¶…æ—¶ï¼Œé¿å…é¡µé¢åŠ è½½æˆ–æ“ä½œè¿‡ç¨‹ä¸­è¢«è¶…æ—¶ä¸­æ–­
        await page.goto(faucet["url"], wait_until="domcontentloaded", timeout=0)
        await asyncio.sleep(2)

        # é˜»å¡ï¼Œç­‰ç”¨æˆ·æ‰‹åŠ¨æ“ä½œåæŒ‰å›è½¦
        input(">>> æŒ‰å›è½¦é”®ç»“æŸæ¼”ç¤ºå¹¶å…³é—­æµè§ˆå™¨ ... ")

    print("æ¼”ç¤ºç»“æŸã€‚å¯æ ¹æ®ä½ åˆšæ‰çš„æ“ä½œï¼ŒæŠŠæ­¥éª¤ï¼ˆæŒ‰é’®æ–‡å­—ã€é€‰æ‹©å™¨ï¼‰å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šæ›´æ–° config ä¸­çš„ stepsã€‚\n")


async def main():
    # æ”¯æŒ --manual / --demoï¼šåªæ‰“å¼€æµè§ˆå™¨ï¼Œç­‰ä½ æ‰‹åŠ¨æ“ä½œä¸€éåæŒ‰å›è½¦
    if "--manual" in sys.argv or "--demo" in sys.argv:
        await run_manual_demo()
        return

    claimer = FaucetClaimer()
    await claimer.run()


if __name__ == "__main__":
    asyncio.run(main())